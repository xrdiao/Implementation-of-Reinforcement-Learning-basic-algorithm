# 过程和思路

## 问题

先列出我遇到的问题吧，这主要存在于以下几点：

- 引用公式时，没有关注具体的变量是什么，这导致编写代码时，引起一些看不见的错误。
- 一个经典的问题：**参数设置**，不收敛的时候需要考虑 `learning_rate` 是否过大，需不需要设置 `weight_decay` 等参数。
- **optimizer的使用**问题，可以回顾 **PPO_Penalty** 的82行，那个注释说明了一切。
- buffer记忆模块的运行流程不是很清楚。
- 损失函数设计（mean的使用）不是很清楚。
- 训练时训练次数增加反而不收敛的问题没有解释。
- 没有思考 **Exploration-Exploitation**，需要进一步考虑这个问题。
- 所有**变量 `shape` 统一**的问题，曾因变量 `advantages` 不统一导致损失函数计算错误，令PPO_continuous不收敛，以后需注意。
- **变量类型**的问题，连续是 `float`， 离散是 `long`。在实现**PPOContinuous**时，因动作类型未从 `long` 转成 `float`，出现了bug。
- loss和reward的走势
- dict{}？什么问题，以后要多试试不同种类的数据存储类型，如：`dict` 不行的时候，去试试 `list` 或者用不同的方式保存。

## 过程和思路

### Q-learning

创建一个Q值表，然后通过这行公式更新表格：`Q(s,a) = Q(s,a) + alpha * (reward + gamma * max(Q(s',a)) - Q(s,a))`。实现的时候是一次动作一次更新表格，没有什么额外的trick。

### Sarsa

跟Q-learning一样的，只是更新公式变成：`Q(s,a) = Q(s,a) + alpha * (reward + gamma * Q(s',a') - Q(s,a)), a' = pi(a|s)`。因此实现时，只需要继承Q-learning，然后修改更新部分即可。

### DQN

使用的是off-line更新，将过去的数据通过一个buffer类（本质是 `deque()`）保存，保存的格式是 `(s,a,r,s',done)`。当agent与环境交互一定次数（buffer类保存一定数量数据）后，开始从buffer中随机采样一个batch用以更新网络参数。更新时采用了双网络更新，一个网络 `target` 的参数固定不动，另一个网络 `eval `的参数会被更新，在`eval`的更新次数达到200时，`eval`会把自身参数共享给`target`。**需要注意动作是怎么选取的，这个因环境不同而不同。**

**更新过程：**通过`target`生成Q值的目标 `reward_ + self.gamma * next_value.view(-1, 1).detach() * (1 - dones_)`，然后通过求一个batch中的平方差期望计算 `loss = torch.mean(F.mse_loss(target, value)).to(self.device)`，并更新网络。

`mean()` 函数通常用来求损失的期望，这在接下来的其他强化学习算法实现中有展现。笼统来说，其他所有方法的父类都是DQN，只是在训练过程上不一样。

### DQN_PER

为什么一开始用 `dict` 作为储存类型，连均匀采样都无法收敛？最后改成直接以 `list` 形式保存才正常运行，难道 `dict` 会搞乱数据顺序？

![image-20240408102348952](C:\Users\Xingrong Diao\AppData\Roaming\Typora\typora-user-images\image-20240408102348952.png)

### DDQN

继承DQN，与DQN不同之处在于DQN只更新一个网络，DDQN需要切换更新两个网络，其他不变。。一个小trick是，可以通过一个全局量来控制更新的网络，如代码中的 `time2swith` 和 `switch`。

### Dueling DQN

改了一下DQN中的网络结构，核心在这个公式 `values + advantages - torch.mean(advantages, dim=1, keepdim=True)`。可以用DDQN或DQN的训练流程来训练。

### PG

需要先从环境中交互一条完整的轨迹并保存下来，实现时没有使用buffer记录经验（可以通过重要性分布实现），即一条轨迹训练一次。更新的过程是根据PG的伪代码来的，也就是这个：

![image-20240401155603873](C:\Users\Xingrong Diao\AppData\Roaming\Typora\typora-user-images\image-20240401155603873.png)

需要弄懂的是，写的loss是没有求导的版本，所以损失函数应该写成：`loss = - (self.alpha * (self.gamma ** t) * G * log_prob)`，而且要注意损失函数需要 **-**（负号）。因为优化器是迭代求损失的最小值，所以需要把损失函数转成正值来进行求解。这一点在使用优势函数后更好解释，因为 $A(s_t,a_t)\leq0$

### PPO-Penalty

训练流程和PG一样，采用了A2C的版本，也就是用优势函数来计算损失。更新过程按照伪代码来，主要问题集中在以下两个变量的计算：

#### GAE的推导

首先需要明白的是，这个优势函数是用在求梯度上的，也就是说 $V(s_t)$ 是一个baseline，对梯度的期望不产生影响：$A(s_t,a_t)=Q(s_t,a_t)-V(s_t)$。

用 $Q(s_t,a_t)=r_t+\gamma V(s_{t+1})$ 替换得到 $\delta_t=A(s_t,a_t)=r_t+\gamma V(s_{t+1})-V(s_t)$，$\delta_t$ 只是做个变量替换。

由于需要估计 $A(s_t,a_t)$，所以需要使用收集到的数据来更新，但是问题在于我们需要用到**多少个step**的数据量，因此定义 $A^k(t)$ 表示在 $t$ 时刻往前看 $k$ 个step的评估，则：
$$ {<div align="left">}
A^1(t)=r_t+\gamma V(s_{t+1})-V(s_t)=\delta^t\\
A^2(t)=r_t+\gamma r_{t+1} + \gamma^2V(s_{t+2})-V(s_t)+\gamma V(s_{t+1}) - V(s_t)+\gamma V(s_{t+1}) = \delta_t+\gamma\delta_{t+1}\\
A^3(t) = ...=\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2}\\
A^k(t)=\sum_{i=0}^{k-1}\gamma^i\delta_{t+i}=\sum_{i=0}^{k-1}\gamma^ir_{t+i}+\gamma^kV(s_{t+k})-V(s_t)
$$ {</div>}


当 $k\rightarrow \infty$，有 $A^k(t)=\sum_{i=0}^{\infty}\gamma^i\delta_{t+i}=\sum_{i=0}^{k-1}\gamma^ir_{t+i}-V(s_t)$，这就是伪代码中的估计由来。但是对于 $A^k(t)$ 来说：

- **$k$ 越大，观测值越多，估计值越少，那偏差越小，方差越大；**
- **$k$ 越小，观测值越少，估计值越多，那偏差越大，方差越小；**

所以对不同 $A^k(t)$ 通过加权求和来估计 $A(t)$，并用 $\hat{A}_t$ 来表示。
$$
\hat{A}_t=A^1_t+\lambda A^2_t+\lambda^2A^3_t+...=\delta_t+\lambda(\delta_t+\gamma\delta_{t+1})+\lambda^2(\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2})+...\\
=\delta_t(1+\lambda+\lambda^2+...)+\gamma\delta_{t+1}(\lambda+\lambda^2+...)+\gamma^2\delta_{t+2}(\lambda^2+...)+...
$$
假设 $\lambda\in[0,1)$ 通过等比求和可以求得
$$
\hat{A}_t = \delta_t(\frac{1-\lambda^k}{1-\lambda})+\gamma\delta_{t+1}(\frac{\lambda(1-\lambda^{k-1})}{1-\lambda})+...
$$
当$k\rightarrow \infty$，有
$$
\hat{A}_t = \delta_t(\frac{1}{1-\lambda})+\gamma\delta_{t+1}(\frac{\lambda}{1-\lambda})+...
$$
因为 $1-\lambda$ 是常数，在更新中相当于步长，可以省略，所以有
$$
\hat{A}_t=\delta_t+\gamma\lambda\delta_{t+1}+\gamma^2\lambda^2\delta_{t+2}+...=\sum_{i=0}^{\infty}(\gamma\lambda)^i\delta_{t+i}
$$
结束。用最后这个结果就能估计出一条轨迹中各个状态的优势函数：
$$
\hat{A}_T=\delta_T\\
\hat{A}_{T-1}=\delta_{T-1}+\gamma\lambda\delta_{T}=\delta_{T-1}+\gamma\lambda\hat{A}_T\\
...\\
\hat{A}_0=\sum_{i=0}^{T}(\gamma\lambda)^i\delta_{i}=\sum_{i=0}^{T}(\gamma\lambda)^i\hat{A}_{i}
$$


也就是说，可以通过倒序的方式完成所有状态的优势函数估计。

具体看这篇知乎：https://zhuanlan.zhihu.com/p/675309680

#### 离散KL散度的计算

$D_{KL}(P||Q) = \sum_i P(i)ln\frac{P(i)}{Q(i)}$。直接调用 `F.kl_div()` 出现了一些奇怪的情况，对训练速度没要求的话建议自己写一个函数。

#### 注意事项

需要弄清楚哪个部分用的是什么概率（其实只需要原始概率，对数概率只是方便计算 `exp(log(a)-log(b))=a/b`）

### PPO-Clip

#### 离散动作

训练流程和PG一样，和PPO-Penalty相比少了个KL散度的计算，整体编写更加简单，但是效果更好。

#### 连续动作

将离散动作中使用的分类分布 `torch.distributions.Categorical()` 更改为正态分布 `torch.distributions.Normal()`，其余不变。

#### 问题

奇了怪，怎么连续动作不收敛。wdf，动作为什么是 `long` 类型？以后**解决bug**必须得一个方法一个方法按流程点进去看，谁能想到最后问题出在**数据类型**上？

在实现的过程中，连续动作PPO出现了报错为 `nan` 的情况，这是由于神经网络输出的方差为0，令正态分布计算出现问题导致的。后经排查，连续动作PPO不收敛是因为变量 `advantages` 的 `shape` 与其他变量的 `shape` 不统一的问题，通过 `view(-1, 1)` 解决。这很奇怪，按理说，这个问题应该会导致离散动作不收敛，但是离散动作PPO却收敛了。

### DDPG

DDPG是由AC结构的DQN，为了解决连续动作问题而提出的，只需要把target中的 $\max_{a'}Q(s',a')$ 换成 $Q(s',a(s';\theta))$，同样需要对actor进行训练。

<img src="C:\Users\Xingrong Diao\AppData\Roaming\Typora\typora-user-images\image-20240406153058237.png" alt="image-20240406153058237" style="zoom:65%;" />

