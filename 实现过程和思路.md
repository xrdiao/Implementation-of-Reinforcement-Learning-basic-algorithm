# 过程和思路

## 问题

先列出我遇到的问题吧，这主要存在于以下几点：

- 公式引用时，没有关注具体的变量是什么，这导致编写代码时，引起一些看不见的错误。
- 一个经典的问题：参数设置，不收敛的时候需要考虑`learning_rate`是否过大，需不需要设置`weight_decay`等参数。
- optimizer的使用问题，可以回顾 'PPO_Penalty' 的82行，那个注释说明了一切。
- buffer记忆模块的运行流程不是很清楚。
- 损失函数设计（mean的使用）不是很清楚，advantage的获取不是很清楚。
- 训练时训练次数增加反而不收敛的问题没有解释。
- 没有思考 **Exploration-Exploitation**，需要进一步考虑这个问题。

## 过程和思路

### Q-learning

创建一个Q值表，然后通过这行公式更新表格：`Q(s,a) = Q(s,a) + alpha * (reward + gamma * max(Q(s',a)) - Q(s,a))`。实现的时候是一次动作一次更新表格，没有什么额外的trick。

### Sarsa

跟Q-learning一样的，只是更新公式变成：`Q(s,a) = Q(s,a) + alpha * (reward + gamma * Q(s',a') - Q(s,a)), a' = pi(a|s)`。因此实现时，只需要继承Q-learning，然后修改更新部分即可。

### DQN

使用的是off-line更新，将过去的数据通过一个buffer类（本质是 `deque()`）保存，保存的格式是 `(s,a,r,s',done)`。当agent与环境交互一定次数（buffer类保存一定数量数据）后，开始从buffer中随机采样一个batch用以更新网络参数。更新时采用了双网络更新，一个网络 `target` 的参数固定不动，另一个网络 `eval `的参数会被更新，在`eval`的更新次数达到200时，`eval`会把自身参数共享给`target`。**需要注意动作是怎么选取的，这个因环境不同而不同。**

**更新过程：**通过`target`生成Q值的目标 `reward_ + self.gamma * next_value.view(-1, 1).detach() * (1 - dones_)`，然后通过求一个batch中的平方差期望计算 `loss = torch.mean(F.mse_loss(target, value)).to(self.device)`，并更新网络。

`mean()` 函数通常用来求损失的期望，这在接下来的其他强化学习算法实现中有展现。笼统来说，其他所有方法的父类都是DQN，只是在训练过程上不一样。

### DDQN

继承DQN，与DQN不同之处在于DQN只更新一个网络，DDQN需要切换更新两个网络，其他不变。。一个小trick是，可以通过一个全局量来控制更新的网络，如代码中的 `time2swith` 和 `switch`。

### Dueling DQN

改了一下DQN中的网络结构，核心在这个公式 `values + advantages - torch.mean(advantages, dim=1, keepdim=True)`。可以用DDQN或DQN的训练流程来训练。

### PG

需要先从环境中交互一条完整的轨迹并保存下来，实现时没有使用buffer记录经验（可以通过重要性分布实现），即一条轨迹训练一次。更新的过程是根据PG的伪代码来的，也就是这个：

![image-20240401155603873](C:\Users\Xingrong Diao\AppData\Roaming\Typora\typora-user-images\image-20240401155603873.png)

需要弄懂的是，你写的loss是没有求导的版本，所以损失函数应该写成：`loss = - (self.alpha * (self.gamma ** t) * G * log_prob)`，而且要注意损失函数需要 **-**（负号）。因为优化器是迭代求损失的最小值，所以需要把损失函数转成正值来进行求解。

### PPO-Penalty

训练流程和PG一样，采用了A2C的版本，也就是用优势函数来计算损失。更新过程按照伪代码来，主要问题集中在以下两个变量的计算：

优势函数的计算：论文中写了个 $\hat{A}=\sum_{t'>t}\gamma^{t'-t}r_{t'}-V_\phi(s_t)$，但这没法转成代码啊。所以需要推导：首先 $t$ 是当前时刻，$t'$ 是未来时刻。

离散KL散度的计算：$D_{KL}(P||Q) = \sum_i P(i)ln\frac{P(i)}{Q(i)}$。直接调用 `F.kl_div()` 出现了一些奇怪的情况，对训练速度没要求的话建议自己写一个函数。

需要弄清楚哪个部分用的是什么概率（其实只需要原始概率，对数概率只是方便计算 `exp(log(a)-log(b))=a/b`）

### PPO-Clip

训练流程和PG一样，和PPO-Penalty相比少了个KL散度的计算，整体编写更加简单，但是效果更好。

### DDPG